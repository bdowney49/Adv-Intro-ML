{"cells":[{"cell_type":"markdown","source":["# EE 596 Mini Project 2\n","Blake Downey and Ryan Maroney"],"metadata":{"id":"ZgVYuTmHnGC4"}},{"cell_type":"markdown","source":["## Data Loading and Imports"],"metadata":{"id":"QBBggRLnCN6N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCq4xcuMRimk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647451097315,"user_tz":420,"elapsed":9237,"user":{"displayName":"Ryan Maroney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvVgWb0xVAbxp-r2NpSHL-Li2AU63Z7m5DmRENmA=s64","userId":"14479519569037289414"}},"outputId":"01f550a0-91ce-4b35-e652-e510a5c334e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","usage: kaggle datasets [-h]\n","                       {list,files,download,create,version,init,metadata,status}\n","                       ...\n","kaggle datasets: error: argument command: invalid choice: 'list1' (choose from 'list', 'files', 'download', 'create', 'version', 'init', 'metadata', 'status')\n","Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","Downloading twitter-sentiment-analysis-winter-2022.zip to /content\n"," 71% 57.0M/80.8M [00:00<00:00, 70.5MB/s]\n","100% 80.8M/80.8M [00:00<00:00, 101MB/s] \n","Archive:  twitter-sentiment-analysis-winter-2022.zip\n","  inflating: twitter_sentiments_data.csv  \n","  inflating: twitter_sentiments_evaluation.csv  \n"]}],"source":["'''\n","Alternatively we can download directly from kaggle and don't need to mount the drive, but either should be fine. With Kaggle we would just need to update the key if I were to reset it for some reason, but this seemed to work for me\n","Source for instructions - https://www.kaggle.com/general/74235\n","Generated kaggle.json using echo rather than uploading it\n","'''\n","! pip install -q kaggle\n","! mkdir ~/.kaggle\n","! echo \"{\\\"username\\\":\\\"ryanmaroney\\\",\\\"key\\\":\\\"b6917ffadd6b2af593b6802bb6721ace\\\"}\" > ~/.kaggle/kaggle.json\n","! kaggle datasets list1\n","! kaggle competitions download -c twitter-sentiment-analysis-winter-2022\n","! unzip twitter-sentiment-analysis-winter-2022.zip\n","#! unzip twitter_sentiments_data.csv.zip\n","#! unzip twitter_sentiments_evaluation.csv.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFhjHuWjSQWm"},"outputs":[],"source":["%%capture\n","\n","import pandas as pd\n","import re, string\n","from nltk.tokenize import TweetTokenizer as TT\n","import nltk\n","import numpy as np\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n","from sklearn.feature_extraction.text import CountVectorizer as count_vec\n","\n","from sklearn.model_selection import train_test_split as tts\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier as RF\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","\n","#!pip install tensorflow\n","import tensorflow\n","import tensorflow_hub as hub\n","!pip install tensorflow-text\n","import tensorflow_text as text\n","from keras.models import Sequential\n","from keras import layers\n","from tensorflow.keras.optimizers import SGD\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.metrics import Precision, Recall\n","\n","# remove if we dont end up using stopwords\n","nltk.download('stopwords')\n","STOP_WORDS  = nltk.corpus.stopwords.words('english')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPN6uDshUJtK"},"outputs":[],"source":["# paths for train/val data and kaggle comp data\n","data_path = './twitter_sentiments_data.csv'\n","eval_path = './twitter_sentiments_evaluation.csv'\n","\n","df_full = pd.read_csv(data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647451120799,"user":{"displayName":"Ryan Maroney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvVgWb0xVAbxp-r2NpSHL-Li2AU63Z7m5DmRENmA=s64","userId":"14479519569037289414"},"user_tz":420},"id":"oclKDrZkSbE4","outputId":"afff7096-1c32-4927-9ba1-9306b643c2ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the data is: (1310106, 6)\n","Column Names: Index(['target', 'tweet_index', 'time stamp', 'Unnamed: 3', 'user', 'tweet'], dtype='object')\n","The targets are: [0 1]\n","Value Counts:\n","0    655081\n","1    655025\n","Name: target, dtype: int64\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["    target  tweet_index                    time stamp Unnamed: 3  \\\n","0        0   1467810672  Mon Apr 06 22:19:49 PDT 2009   NO_QUERY   \n","1        0   1467810917  Mon Apr 06 22:19:53 PDT 2009   NO_QUERY   \n","2        0   1467811193  Mon Apr 06 22:19:57 PDT 2009   NO_QUERY   \n","3        0   1467811372  Mon Apr 06 22:20:00 PDT 2009   NO_QUERY   \n","4        0   1467811592  Mon Apr 06 22:20:03 PDT 2009   NO_QUERY   \n","5        0   1467811795  Mon Apr 06 22:20:05 PDT 2009   NO_QUERY   \n","6        0   1467812025  Mon Apr 06 22:20:09 PDT 2009   NO_QUERY   \n","7        0   1467812416  Mon Apr 06 22:20:16 PDT 2009   NO_QUERY   \n","8        0   1467812579  Mon Apr 06 22:20:17 PDT 2009   NO_QUERY   \n","9        0   1467812723  Mon Apr 06 22:20:19 PDT 2009   NO_QUERY   \n","10       0   1467812771  Mon Apr 06 22:20:19 PDT 2009   NO_QUERY   \n","11       0   1467812799  Mon Apr 06 22:20:20 PDT 2009   NO_QUERY   \n","12       0   1467812964  Mon Apr 06 22:20:22 PDT 2009   NO_QUERY   \n","13       0   1467813579  Mon Apr 06 22:20:31 PDT 2009   NO_QUERY   \n","14       0   1467813985  Mon Apr 06 22:20:37 PDT 2009   NO_QUERY   \n","15       0   1467813992  Mon Apr 06 22:20:38 PDT 2009   NO_QUERY   \n","16       0   1467814119  Mon Apr 06 22:20:40 PDT 2009   NO_QUERY   \n","17       0   1467814180  Mon Apr 06 22:20:40 PDT 2009   NO_QUERY   \n","18       0   1467814192  Mon Apr 06 22:20:41 PDT 2009   NO_QUERY   \n","19       0   1467814438  Mon Apr 06 22:20:44 PDT 2009   NO_QUERY   \n","\n","               user                                              tweet  \n","0     scotthamilton  is upset that he can't update his Facebook by ...  \n","1          mattycus  @Kenichan I dived many times for the ball. Man...  \n","2            Karoli  @nationwideclass no, it's not behaving at all....  \n","3          joy_wolf                      @Kwesidei not the whole crew   \n","4           mybirch                                        Need a hug   \n","5   2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n","6           mimismo                          @twittera que me muera ?   \n","7    erinx3leannexo        spring break in plain city... it's snowing   \n","8      pardonlauren                         I just re-pierced my ears   \n","9              TLeC  @caregiving I couldn't bear to watch it.  And ...  \n","10  robrobbierobert  @octolinz16 It it counts, idk why I did either...  \n","11       HairByJess  @iamjazzyfizzle I wish I got to watch it with ...  \n","12   lovesongwriter  Hollis' death scene will hurt me severely to w...  \n","13       starkissed  @LettyA ahh ive always wanted to see rent  lov...  \n","14           quanvu  @alydesigns i was out most of the day so didn'...  \n","15       swinspeedx  one of my friend called me, and asked to meet ...  \n","16        cooliodoc   @angry_barista I baked you a cake but I ated it   \n","17       viJILLante             this week is not going as i had hoped   \n","18       Ljelli3166                         blagh class at 8 tomorrow   \n","19    ChicagoCubbie     I hate when I have to call and wake people up   "],"text/html":["\n","  <div id=\"df-50e68b91-bd77-44df-9485-a1bc43fa5da6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>tweet_index</th>\n","      <th>time stamp</th>\n","      <th>Unnamed: 3</th>\n","      <th>user</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1467810672</td>\n","      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>scotthamilton</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1467810917</td>\n","      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mattycus</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1467811193</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Karoli</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1467811372</td>\n","      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>joy_wolf</td>\n","      <td>@Kwesidei not the whole crew</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1467811592</td>\n","      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mybirch</td>\n","      <td>Need a hug</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>1467811795</td>\n","      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>2Hood4Hollywood</td>\n","      <td>@Tatiana_K nope they didn't have it</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>1467812025</td>\n","      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mimismo</td>\n","      <td>@twittera que me muera ?</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>1467812416</td>\n","      <td>Mon Apr 06 22:20:16 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>erinx3leannexo</td>\n","      <td>spring break in plain city... it's snowing</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>1467812579</td>\n","      <td>Mon Apr 06 22:20:17 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>pardonlauren</td>\n","      <td>I just re-pierced my ears</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>1467812723</td>\n","      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>TLeC</td>\n","      <td>@caregiving I couldn't bear to watch it.  And ...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0</td>\n","      <td>1467812771</td>\n","      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>robrobbierobert</td>\n","      <td>@octolinz16 It it counts, idk why I did either...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0</td>\n","      <td>1467812799</td>\n","      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>HairByJess</td>\n","      <td>@iamjazzyfizzle I wish I got to watch it with ...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>1467812964</td>\n","      <td>Mon Apr 06 22:20:22 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>lovesongwriter</td>\n","      <td>Hollis' death scene will hurt me severely to w...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0</td>\n","      <td>1467813579</td>\n","      <td>Mon Apr 06 22:20:31 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>starkissed</td>\n","      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0</td>\n","      <td>1467813985</td>\n","      <td>Mon Apr 06 22:20:37 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>quanvu</td>\n","      <td>@alydesigns i was out most of the day so didn'...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0</td>\n","      <td>1467813992</td>\n","      <td>Mon Apr 06 22:20:38 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>swinspeedx</td>\n","      <td>one of my friend called me, and asked to meet ...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0</td>\n","      <td>1467814119</td>\n","      <td>Mon Apr 06 22:20:40 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>cooliodoc</td>\n","      <td>@angry_barista I baked you a cake but I ated it</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0</td>\n","      <td>1467814180</td>\n","      <td>Mon Apr 06 22:20:40 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>viJILLante</td>\n","      <td>this week is not going as i had hoped</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0</td>\n","      <td>1467814192</td>\n","      <td>Mon Apr 06 22:20:41 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Ljelli3166</td>\n","      <td>blagh class at 8 tomorrow</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0</td>\n","      <td>1467814438</td>\n","      <td>Mon Apr 06 22:20:44 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>ChicagoCubbie</td>\n","      <td>I hate when I have to call and wake people up</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50e68b91-bd77-44df-9485-a1bc43fa5da6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-50e68b91-bd77-44df-9485-a1bc43fa5da6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-50e68b91-bd77-44df-9485-a1bc43fa5da6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["'''\n","Quick Data Analysis\n","'''\n","print(f'Shape of the data is: {df_full.shape}')\n","print(f'Column Names: {df_full.columns}')\n","print(f'The targets are: {df_full.target.unique()}')\n","print(f'Value Counts:\\n{df_full[\"target\"].value_counts()}')\n","print()\n","\n","\n","#Seems like the only important columns are target and tweet. \n","df_full.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1647451120800,"user":{"displayName":"Ryan Maroney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvVgWb0xVAbxp-r2NpSHL-Li2AU63Z7m5DmRENmA=s64","userId":"14479519569037289414"},"user_tz":420},"id":"mxhISwocagK-","outputId":"ac4e8bb9-48e5-478e-cec9-732f9adbbe9f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   target                                              tweet\n","0       0  is upset that he can't update his Facebook by ...\n","1       0  @Kenichan I dived many times for the ball. Man...\n","2       0  @nationwideclass no, it's not behaving at all....\n","3       0                      @Kwesidei not the whole crew \n","4       0                                        Need a hug "],"text/html":["\n","  <div id=\"df-914a075b-cf07-4b81-9c8c-3394b0a68f8c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>@Kwesidei not the whole crew</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>Need a hug</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-914a075b-cf07-4b81-9c8c-3394b0a68f8c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-914a075b-cf07-4b81-9c8c-3394b0a68f8c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-914a075b-cf07-4b81-9c8c-3394b0a68f8c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["df_important = df_full[['target','tweet']]\n","tweets_test = df_important['tweet']\n","df_important.head()"]},{"cell_type":"markdown","source":["## Preprocessing"],"metadata":{"id":"LxHMjtYLmZ5Q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WorNeOzYmuP_"},"outputs":[],"source":["'''\n","Functions to translate common twitter slang into actual english\n","\n","Refernces on some of the common slang: \n","https://saveti.kombib.rs/twerminology-twitter-slang-words-and-abbreviations#:~:text=egotwistical%3A%20Slang%20term%20used%20to%20describe%20a%20user,Shorthand%20versions%20of%20%22e-mail%22%20routinely%20used%20on%20Twitter.\n","https://slangit.com/terms/twitter\n","\n","'''\n","\n","tokenizer = TT(reduce_len=True)\n","\n","\n","def translate_slang(token):\n","  if token == 'u' or token == 'ya' : return 'you'\n","  elif token == 'n' : return 'and'\n","  elif token == 'ur' : return 'your'\n","  elif token == 'r' : return 'are'\n","  elif token == 'yrs' : return 'years'\n","  elif token == '2day' or token == '2-day' : return 'today'\n","  elif token == '2morow' or token == '2moro' : return 'tomorrow'\n","  elif token == '2nite' or token == '2-nite' : return 'tonight'\n","  elif token == 'some1' or token == 'sum1' : return 'someone'\n","  elif token == 'any1' : return 'anyone'\n","  elif token == 'pls' or token == 'plss' or token == 'plz' or token == 'plzz' or token == 'plzzz' : return 'please'\n","  elif token == 'ab' or token == 'abt' or token == 'bout' : return 'about'\n","  elif token == 'b4' or token == 'bfor' : return 'before'\n","  elif token == 'bc' : return 'because'\n","  elif token == 'cld' : return 'could'\n","  elif token == 'clk' : return 'click'\n","  elif token == 'chk' : return 'check'\n","  elif token == 'cre8' : return 'create'\n","  elif token == 'l8ter' : return 'later'\n","  elif token == 'da' : return 'the'\n","  elif token == 'deets' : return 'details'\n","  elif token == 'fab' : return 'fabulous'\n","  elif token == 'fav' : return 'favorite'\n","  elif token == 'oh' : return 'overheard'\n","  elif token == 'rt' : return 'retweet'\n","  elif token == 'sp' : return 'sponsored'\n","  elif token == 'wuz' or token == 'woz' : return 'was'\n","  elif token == 'wtv' : return 'whatever'\n","  elif token == 'sp' : return 'sponsored'\n","  elif token == 'w' : return 'with'\n","  elif token == 'oop' : return 'embarassed'\n","  elif token == 'sry' or token == 'srry' : return 'sorry'\n","  elif token == 'kk' or token == 'kewl' : return 'cool'\n","  elif token == 'puter' or token == \"'puter\" : return 'computer'\n","  elif token == 'hahaha' or token == 'hahahaha' : return 'haha'\n","  elif token == 'til' or token == 'till' or token == \"'till\" : return 'until'\n","  elif token == 'wut' or token == 'wat' or token == 'wah' : return 'what'\n","  elif token == 'fax': return 'truth'\n","  elif token == 'cap': return 'lie'\n","  elif token == 'tho': return 'though'\n","  elif token == 'thru': return 'through'\n","  elif token == 'cx': return 'correction'\n","  elif token == 'gn': return 'goodnight'\n","  elif token == 'tl': return 'timeline'\n","  elif token == 'nvm': return 'nevermind'\n","  elif token == 'aaw' or token == 'aww': return 'aw'\n","  elif token == 'fuk' or token == 'fuq': return 'fuck'\n","  elif token == 'helloo' or token == 'hellooo' : return 'hello'\n","\n","  return token\n","\n","def special_case(tweet):\n","  tweet = tweet.split()\n","  sent = []\n","  for w in tweet:\n","    if w == 'b/c':\n","      sent.append('because')\n","    elif w == 'w/o' or w == 'w/out':\n","      sent.append('with')\n","      sent.append('out')\n","    elif  w == 'w/u' or w == 'w/you':\n","      sent.append('with')\n","      sent.append('you')\n","    else: sent.append(w)\n","  return ' '.join(sent)\n","\n","\n","def translate(tweet):\n","  out = [] #list of words\n","\n","  tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", tweet) # remove mentions\n","  tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', tweet)\n","  #handle special cases before tokenizer ie words like 'b/c' that have a '/' in it\n","  tweet = special_case(tweet)\n","  words = tokenizer.tokenize(tweet) # use the tokenizer to split the words/punctuations \n","  \n","  # translate abbreviations into multi words\n","  for word in words:\n","    if word not in string.punctuation:# and word not in STOP_WORDS:\n","      word = word.lower()\n","      if word == 'idk':\n","        out.append('i')\n","        out.append('dont')\n","        out.append('know')\n","      elif word == 'wcw':\n","        out.append('women')\n","        out.append('crush')\n","        out.append('wednesday')\n","      elif word == 'mcm':\n","        out.append('man')\n","        out.append('crush')\n","        out.append('monday')\n","      elif word == 'gonna' or word == 'gona':\n","        out.append('going')\n","        out.append('to')\n","      elif word == 'ftb':\n","        out.append('fuck')\n","        out.append('that')\n","        out.append('bitch')\n","      elif word == 'fym':\n","        out.append('fuck')\n","        out.append('you')\n","        out.append('mean')\n","      elif word == 'gtfo':\n","        out.append('get')\n","        out.append('the')\n","        out.append('fuck')\n","        out.append('out')\n","      elif word == 'nbd':\n","        out.append('no')\n","        out.append('big')\n","        out.append('deal')\n","      elif word == 'stfu':\n","        out.append('shut')\n","        out.append('the')\n","        out.append('fuck')\n","        out.append('up')\n","      elif word == 'fml':\n","        out.append('fuck')\n","        out.append('my')\n","        out.append('life')\n","      elif word == 'lol':\n","        out.append('laugh')\n","        out.append('out')\n","        out.append('loud')\n","      elif word == 'rofl':\n","        out.append('rolling')\n","        out.append('on')\n","        out.append('the')\n","        out.append('floor')\n","        out.append('laughing')\n","      elif word == 'nsfw':\n","        out.append('not')\n","        out.append('safe')\n","        out.append('for')\n","        out.append('work')\n","      elif word == 'sfw':\n","        out.append('safe')\n","        out.append('for')\n","        out.append('work')\n","      elif word == 'fwb':\n","        out.append('friends')\n","        out.append('with')\n","        out.append('benefits')\n","      elif word == 'fyi':\n","        out.append('for')\n","        out.append('your')\n","        out.append('information')\n","      elif word == 'nm':\n","        out.append('not')\n","        out.append('much')\n","      elif word == 'brb':\n","        out.append('be')\n","        out.append('right')\n","        out.append('back')\n","      elif word == 'ikr':\n","        out.append('i')\n","        out.append('know')\n","        out.append('right')\n","      elif word == 'pos':\n","        out.append('piece')\n","        out.append('of')\n","        out.append('shit')\n","      elif word == 'ootd':\n","        out.append('outfit')\n","        out.append('of')\n","        out.append('the')\n","        out.append('day')\n","      elif word == 'lmao':\n","        out.append('laugh')\n","        out.append('my')\n","        out.append('ass')\n","        out.append('off')\n","      elif word == 'hags':\n","        out.append('have')\n","        out.append('a')\n","        out.append('great')\n","        out.append('summer')\n","      elif word == 'jk':\n","        out.append('just')\n","        out.append('kidding')\n","      elif word == 'tbt':\n","        out.append('throwback')\n","        out.append('thursday')\n","      elif word == 'rn':\n","        out.append('right')\n","        out.append('now')\n","      elif word == 'tbh':\n","        out.append('to')\n","        out.append('be')\n","      elif word == 'gl':\n","        out.append('good')\n","        out.append('luck')\n","        out.append('honest')\n","      elif word == 'tftf':\n","        out.append('thanks')\n","        out.append('for')\n","        out.append('the')\n","        out.append('follow')\n","      elif word == 'tfti':\n","        out.append('thanks')\n","        out.append('for')\n","        out.append('the')\n","        out.append('invite')\n","      elif word == 'smh':\n","        out.append('shake')\n","        out.append('my')\n","        out.append('head')\n","      elif word == 'smh':\n","        out.append('shake')\n","        out.append('my')\n","        out.append('head')\n","      elif word == 'idc':\n","        out.append('i')\n","        out.append('dont')\n","        out.append('care')\n","      elif word == 'mfs':\n","        out.append('mother')\n","        out.append('fuckers')\n","      elif word == 'fr':\n","        out.append('for')\n","        out.append('real')\n","      elif word == 'wtf':\n","        out.append('what')\n","        out.append('the')\n","        out.append('fuck')\n","      elif word == 'mtf':\n","        out.append('more')\n","        out.append('to')\n","        out.append('follow')\n","      elif word == 'nts':\n","        out.append('note')\n","        out.append('to')\n","        out.append('self')\n","      elif word == 'ic':\n","        out.append('i')\n","        out.append('see')\n","      elif word == 'ftl':\n","        out.append('for')\n","        out.append('the')\n","        out.append('loss')\n","      elif word == 'ftw':\n","        out.append('for')\n","        out.append('the')\n","        out.append('win')\n","      elif word == 'fomo':\n","        out.append('fear')\n","        out.append('of')\n","        out.append('missing')\n","        out.append('out')\n","      elif word == 'f2f' or word == 'ftf':\n","        out.append('face')\n","        out.append('to')\n","        out.append('face')\n","      elif word == 'irl':\n","        out.append('in')\n","        out.append('real')\n","        out.append('life')\n","      elif word == 'tmi':\n","        out.append('to')\n","        out.append('much')\n","        out.append('information')\n","      elif word == 'omg':\n","        out.append('ohh')\n","        out.append('my')\n","        out.append('gosh')\n","      elif word == 'psa':\n","        out.append('public')\n","        out.append('service')\n","        out.append('announcement')\n","      elif word == 'abg':\n","        out.append('asian')\n","        out.append('baby')\n","        out.append('girl')\n","      elif word == 'abb':\n","        out.append('asian')\n","        out.append('baby')\n","        out.append('boy')\n","      elif word == 'tfw':\n","        out.append('that')\n","        out.append('feeling')\n","        out.append('when')\n","      elif word == 'twart':\n","        out.append('twitter')\n","        out.append('art')\n","      elif word == 'yolo':\n","        out.append('you')\n","        out.append('only')\n","        out.append('live')\n","        out.append('once')\n","      elif word == 'yoyo':\n","        out.append(\"you\")\n","        out.append(\"are\")\n","        out.append('on')\n","        out.append('your')\n","        out.append('own')\n","      elif word == 'hifw':\n","        out.append('how')\n","        out.append('i')\n","        out.append('felt')\n","        out.append('when')\n","      elif word == 'ngl':\n","        out.append('not')\n","        out.append('going')\n","        out.append('to')\n","        out.append('lie')\n","      elif word == 'icymi':\n","        out.append('in')\n","        out.append('case')\n","        out.append('you')\n","        out.append('missed')\n","        out.append('it')\n","      elif word == 'wouldnt' or word == \"wouldn't\":\n","        out.append('would')\n","        out.append('not')\n","      elif word == 'cant' or word == \"can't\":\n","        out.append('can')\n","        out.append('not')\n","      elif word == 'didnt' or word == \"didn't\":\n","        out.append('did')\n","        out.append('not')\n","      elif word == 'couldnt' or word == \"couldn't\":\n","        out.append('could')\n","        out.append('not')\n","      elif word == 'wo':\n","        out.append('with')\n","        out.append('out')\n","      elif word == 'imy' or word == 'imu':\n","        out.append('i')\n","        out.append('miss')\n","        out.append('you')\n","      elif word == 'ily' or word == 'ilyy':\n","        out.append('i')\n","        out.append('love')\n","        out.append('you')\n","      elif word == 'wth':\n","        out.append('what')\n","        out.append('the')\n","        out.append('hell')\n","      elif word == 'pc':\n","        out.append('politically')\n","        out.append('correct')\n","      elif word == 'imo':\n","        out.append('in')\n","        out.append('my')\n","        out.append('opinion')\n","      elif word == 'sob':\n","        out.append('son')\n","        out.append('of')\n","        out.append('a')\n","        out.append('bitch')\n","      elif word == 'wanna' or word == 'wana':\n","        out.append('want')\n","        out.append('to')\n","      elif word == 'rip':\n","        out.append('rest')\n","        out.append('in')\n","        out.append('peace')\n","      elif word == 'dm' or word == 'dms' or word == \"dm's\":\n","        out.append('direct')\n","        out.append('message')\n","      elif word == 'f4f':\n","        out.append('follow')\n","        out.append('for')\n","        out.append('follow')\n","      elif word == 'atm':\n","        out.append('at')\n","        out.append('the')\n","        out.append('moment')\n","      elif word == \"i'm\":\n","        out.append('i')\n","        out.append('am')\n","      elif word == \"it's\" or word == 'its':\n","        out.append('it')\n","        out.append('is')\n","      elif word == \"i've\":\n","        out.append('i')\n","        out.append('have')\n","      elif word == 'el-stupido':\n","        out.append('the')\n","        out.append('stupid')\n","      elif word == '..' or word == '...' :\n","        continue\n","      else: \n","        out.append(word)\n","\n","  # call translate slang to convert abbreviated words to single words\n","  for i,word in enumerate(out):\n","    out[i] = translate_slang(word)\n","\n","  return ' '.join(out) #return out if you want tokenized output - otherwise return the joined list of words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1647451122653,"user":{"displayName":"Ryan Maroney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvVgWb0xVAbxp-r2NpSHL-Li2AU63Z7m5DmRENmA=s64","userId":"14479519569037289414"},"user_tz":420},"id":"RGLjPPVquIk2","outputId":"db045f38-ffb8-46ab-a304-4556cb643b06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing on sample sentence...\n","test sentence: hellooooo hahaha wtf i cant go w/o u to da party, sry bout it\n","translated: hello haha what the fuck i can not go with out you to the party sorry about it\n","\n","\n","Testing on actual twitter data...\n","original: ugh. cant sleep. its 1:30am. \n","translated: ugh can not sleep it is 1:30 am\n","\n","original: Hanging in Crooners. Wanna sing. Can't. Sucks. \n","translated: hanging in crooners want to sing can not sucks\n","\n","original: @eRRe_sC aaw i miss ya all too.. im leaving to BH tomorrow &quot;morning&quot; i think.. aww i wanna go to the beach w u girls!! \n","translated: aw i miss you all too im leaving to bh tomorrow morning i think aw i want to go to the beach with you girls\n","\n","original: Is pissed off that there's no ASBA's for a radio station. \n","translated: is pissed off that there's no asba's for a radio station\n","\n","original: wednesday my b-day n don't know what 2 do!  \n","translated: wednesday my b-day and don't know what 2 do\n","\n","original: I know my life has been flipped upside down when I just thought in my head that some Ramen sounds good. \n","translated: i know my life has been flipped upside down when i just thought in my head that some ramen sounds good\n","\n","original: I am in pain. My back and sides hurt. Not to mention crying is made of fail. \n","translated: i am in pain my back and sides hurt not to mention crying is made of fail\n","\n","original: Late night snack, glass of OJ b/c I'm &quot;down with the sickness&quot;, then back to sleep...ugh I hate getting sick... \n","translated: late night snack glass of oj because i am down with the sickness then back to sleep ugh i hate getting sick\n","\n","original: @allyheman but.. but.. but.. I'm not a big fan on camilla belle \n","translated: but but but i am not a big fan on camilla belle\n","\n","original: @grum WAH I can't see clip, must be el-stupido work filters. Can't wait 'till I get a 'puter. Something else 2 blame ex 4. He broke mine \n","translated: what i can not see clip must be the stupid work filters can not wait until i get a computer something else 2 blame ex 4 he broke mine\n","\n"]}],"source":["# testing the above function\n","test_sentence = 'hellooooo hahaha wtf i cant go w/o u to da party, sry bout it'\n","print('Testing on sample sentence...')\n","print(f'test sentence: {test_sentence}')\n","print(f'translated: {translate(test_sentence)}\\n\\n')\n","\n","print('Testing on actual twitter data...')\n","for tweet in tweets_test[100:110]:\n","  print(f'original: {tweet}')\n","  print(f'translated: {translate(tweet)}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BMnvNeKTs5j"},"outputs":[],"source":["# dividing factor\n","sub_sample_ratio = 1\n","\n","# separate to subsample indiviudally\n","df_1 = df_important[df_important['target'] == 1]\n","df_0 = df_important[df_important['target'] == 0]\n","\n","# length of target classes \n","pos_count = len(df_1)\n","neg_count = len(df_0)\n","\n","# subsample the length\n","df_1 = df_1.iloc[:int(pos_count/sub_sample_ratio)]\n","df_0 = df_0.iloc[:int(neg_count/sub_sample_ratio)]\n","\n","# concat so the df has both classes again\n","df = pd.concat([df_0, df_1])\n","\n","# apply the translate function to the tweets\n","df['tweet'] = df['tweet'].apply(translate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rI29cmNnA1zu"},"outputs":[],"source":["SEED = 53\n","\n","# split the data into train and val\n","X_train, X_val, y_train, y_val = tts(df['tweet'], df['target'], test_size=0.2, random_state=SEED)\n","\n","# initiate the vectorizer\n","#vec = tfidf(max_features=100000, ngram_range=[1,4])\n","vec = count_vec(max_features=100000, ngram_range=[1,4])\n","\n","# transform the train/validation data with the tfidf vectorizer\n","X_train = vec.fit_transform(X_train)\n","X_val = vec.transform(X_val)\n"]},{"cell_type":"markdown","source":["## Basline Models"],"metadata":{"id":"jLG9H7SpmsFQ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142896,"status":"ok","timestamp":1647453099040,"user":{"displayName":"Ryan Maroney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvVgWb0xVAbxp-r2NpSHL-Li2AU63Z7m5DmRENmA=s64","userId":"14479519569037289414"},"user_tz":420},"id":"H7azagOGf1jw","outputId":"1cfb2cc5-6720-456e-e73f-8720191f69bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of Train Data: (1048084, 100000)\n","Shape of Validation Data: (262022, 100000)\n","\n","LR Results:\n","Train Accuracy:  84.12%\n","Validation Accuracy:  81.99%\n","Train f1:  0.84398\n","Validation f1:  0.82366\n","\n","SGD Results:\n","Train Accuracy:  82.32%\n","Validation Accuracy:  81.16%\n","Train f1:  0.82628\n","Validation f1:  0.81533\n"]}],"source":["'''\n","BASELINE -- non deep learning\n","\n","Test LR/SGD Classifier with a TfidfVectorizer\n","'''\n","print(f'Shape of Train Data: {X_train.shape}')\n","print(f'Shape of Validation Data: {X_val.shape}')\n","\n","# fit the models\n","model_LR = LogisticRegression(penalty='l2', max_iter=500, C=0.1, random_state=SEED).fit(X_train, y_train)\n","model_SGD = SGDClassifier(penalty='l2').fit(X_train, y_train)\n","\n","# preds for logistic regression\n","train_pred_LR = model_LR.predict(X_train)\n","val_pred_LR = model_LR.predict(X_val)\n","\n","# preds for sgd classifier\n","train_pred_SGD = model_SGD.predict(X_train)\n","val_pred_SGD = model_SGD.predict(X_val)\n","\n","# print results\n","print('\\nLR Results:')\n","print(f'Train Accuracy: {accuracy_score(y_train, train_pred_LR)*100 : .2f}%')\n","print(f'Validation Accuracy: {accuracy_score(y_val, val_pred_LR)*100 : .2f}%')\n","print(f'Train f1: {f1_score(y_train, train_pred_LR): .5f}')\n","print(f'Validation f1: {f1_score(y_val, val_pred_LR): .5f}')\n","\n","print('\\nSGD Results:')\n","print(f'Train Accuracy: {accuracy_score(y_train, train_pred_SGD)*100 : .2f}%')\n","print(f'Validation Accuracy: {accuracy_score(y_val, val_pred_SGD)*100 : .2f}%')\n","print(f'Train f1: {f1_score(y_train, train_pred_SGD): .5f}')\n","print(f'Validation f1: {f1_score(y_val, val_pred_SGD): .5f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdyPTK01HHxD"},"outputs":[],"source":["'''\n","Baseline Results with the evaluation set for kaggle\n","'''\n","\n","df_eval = pd.read_csv(eval_path)\n","\n","df_eval = df_eval[['tweet_index','tweet']]\n","\n","# apply the translate function\n","df_eval['tweet'] = df_eval['tweet'].apply(translate)\n","\n","# apply vectorizer\n","vectorized_eval = vec.transform(df_eval['tweet'])\n","\n","# predict\n","pred_eval = model_LR.predict(vectorized_eval)\n","\n","out_data = {\n","    'tweet_index' : (df_eval['tweet_index']),\n","    'target' : (pred_eval)\n","}\n","\n","out_df = pd.DataFrame(out_data, index=None)\n","\n","out_df.to_csv('preds.csv')"]},{"cell_type":"markdown","source":["## BERT Based Model"],"metadata":{"id":"DaxzX_VomV1x"}},{"cell_type":"code","source":["X_train, X_val, y_train, y_val = tts(df['tweet'], df['target'], test_size=0.2, random_state=SEED)"],"metadata":{"id":"p4WTnq_OxkZO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0u5H5-HIyyJ"},"outputs":[],"source":["'''\n","BERT HERE\n","\n","Reference - https://www.analyticsvidhya.com/blog/2021/12/text-classification-using-bert-and-tensorflow/\n","'''\n","bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n","bert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')"]},{"cell_type":"code","source":["# Bert layers\n","text_input = tensorflow.keras.layers.Input(shape=(), dtype=tensorflow.string, name='text')\n","preprocessed_text = bert_preprocess(text_input)\n","outputs = bert_encoder(preprocessed_text)\n","\n","# Neural network layers\n","l = tensorflow.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n","l = tensorflow.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n","\n","# Use inputs and outputs to construct a final model\n","model = tensorflow.keras.Model(inputs=[text_input], outputs = [l])\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"Dqn8TL0bgvW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os.path\n","\n","# Download pre-trained model if available\n","if os.path.exists(\"test_bert_model_epoch_1_chunk_94.h5\"):\n","  model = tensorflow.keras.models.load_model(\"test_bert_model_epoch_1_chunk_94.h5\", custom_objects={'KerasLayer':hub.KerasLayer})"],"metadata":{"id":"ZVIcwx7ri3Vy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmgpJHdVqpBF","executionInfo":{"status":"ok","timestamp":1647453513149,"user_tz":420,"elapsed":231,"user":{"displayName":"Ryan Maroney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvVgWb0xVAbxp-r2NpSHL-Li2AU63Z7m5DmRENmA=s64","userId":"14479519569037289414"}},"outputId":"27c56d5f-ba22-49e8-9fa3-786ba02b0a1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_5\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," text (InputLayer)              [(None,)]            0           []                               \n","                                                                                                  \n"," keras_layer (KerasLayer)       {'input_word_ids':   0           ['text[0][0]']                   \n","                                (None, 128),                                                      \n","                                 'input_type_ids':                                                \n","                                (None, 128),                                                      \n","                                 'input_mask': (Non                                               \n","                                e, 128)}                                                          \n","                                                                                                  \n"," keras_layer_1 (KerasLayer)     {'pooled_output': (  109482241   ['keras_layer[0][0]',            \n","                                None, 768),                       'keras_layer[0][1]',            \n","                                 'sequence_output':               'keras_layer[0][2]']            \n","                                 (None, 128, 768),                                                \n","                                 'default': (None,                                                \n","                                768),                                                             \n","                                 'encoder_outputs':                                               \n","                                 [(None, 128, 768),                                               \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768)]}                                               \n","                                                                                                  \n"," dropout (Dropout)              (None, 768)          0           ['keras_layer_1[0][13]']         \n","                                                                                                  \n"," output (Dense)                 (None, 1)            769         ['dropout[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,483,010\n","Trainable params: 769\n","Non-trainable params: 109,482,241\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["# BERT has roughly 110 Million parameters, training on only a subset in final notebook for reasonable training time and to demonstrate training\n","subset_percent = 0.05\n","subset_number = int(len(X_train) * subset_percent)\n","model.fit(X_train.iloc[:subset_number], y_train.iloc[:subset_number], validation_data=(X_val,y_val), epochs=1, batch_size = 32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGzTwMW8gvTw","executionInfo":{"status":"ok","timestamp":1647459973329,"user_tz":420,"elapsed":6202468,"user":{"displayName":"Ryan Maroney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvVgWb0xVAbxp-r2NpSHL-Li2AU63Z7m5DmRENmA=s64","userId":"14479519569037289414"}},"outputId":"2a181eb9-cdee-48e5-833c-953daa087f72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1638/1638 [==============================] - 6156s 4s/step - loss: 0.5391 - accuracy: 0.7298 - val_loss: 0.5093 - val_accuracy: 0.7560\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f4823367110>"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["\n","df_eval = pd.read_csv(eval_path)\n","\n","df_eval = df_eval[['tweet_index','tweet']]\n","\n","# apply the translate function\n","df_eval['tweet'] = df_eval['tweet'].apply(translate)\n","tweets = df_eval['tweet']\n","\n","predictions = model.predict(tweets)\n","\n","predictions = np.round(np.array(predictions))\n","\n","out_data = {\n","    'tweet_index' : (df_eval['tweet_index']),\n","    'target' : (predictions.flatten())\n","}\n","\n","out_df = pd.DataFrame(out_data, index=None)\n","\n","out_df.to_csv('bert_predictions.csv')"],"metadata":{"id":"q9nKPII1kFSQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bi-LSTM Based Model"],"metadata":{"id":"rMx0MBjumwV4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcOyen2L9hGN"},"outputs":[],"source":["tokener = Tokenizer(num_words=20000, split=' ') # max 10000 words and split the string on space\n","def token_and_pad(tweets):\n","    tokener.fit_on_texts(tweets)\n","    data = tokener.texts_to_sequences(tweets)\n","    data = pad_sequences(data, padding='post', maxlen=50)\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REUy1zkz9Tjg"},"outputs":[],"source":["# convert to tokens and pad each sequence to the same length\n","data = token_and_pad(df['tweet']) \n","\n","# this makes it go beep boop bop\n","target = pd.get_dummies(df['target']) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kiL3o99BWPZ3","outputId":"fec36735-c55f-4697-c8db-8bebbba3e1d5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647452210074,"user_tz":420,"elapsed":302,"user":{"displayName":"Ryan Maroney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvVgWb0xVAbxp-r2NpSHL-Li2AU63Z7m5DmRENmA=s64","userId":"14479519569037289414"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of X_train: (1179095, 50)\n","Shape of X_val: (131011, 50)\n"]}],"source":["# train test split this bitttttch\n","SEED = 42\n","X_train, X_val, y_train, y_val = tts(data, target, test_size=0.1, random_state=SEED)\n","\n","print(f'Shape of X_train: {X_train.shape}')\n","print(f'Shape of X_val: {X_val.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVe2kr4iNAUG"},"outputs":[],"source":["'''\n","Bi-LSTM\n","'''\n","def build_model():\n","    # bi - lstm deep learning architecture\n","    model = Sequential()\n","    model.add(layers.Embedding(20000, 32, input_length=50))\n","    model.add(layers.Conv1D(500, 3, padding='same', activation='relu'))\n","    model.add(layers.MaxPooling1D())\n","    model.add(layers.Bidirectional(layers.LSTM(128)))\n","    model.add(layers.Dense(500, activation='relu'))\n","    model.add(layers.Dropout(0.58))\n","    model.add(layers.Dense(2, activation='softmax'))\n","\n","    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) #, Precision(), Recall()\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4kFQnFxWPZ4"},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint\n","\n","model_name = './models/best_rmsprop3.epoch{epoch:02d}-val_loss{val_loss:.2f}.h5'\n","\n","chkpoint = ModelCheckpoint(filepath=model_name, \n","                             monitor='val_loss',\n","                             verbose=1, \n","                             save_best_only=True,\n","                             mode='min')\n","\n","callbacks = [chkpoint]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djbeJu-tWPZ4"},"outputs":[],"source":["model = build_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DSyqFkzE0emQ","outputId":"0c1b80cf-7992-44b4-f1f3-ce1a413cf50e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.7989\n","Epoch 00001: val_loss improved from inf to 0.41024, saving model to ./models\\best_rmsprop3.epoch01-val_loss0.41.h5\n","1152/1152 [==============================] - 639s 552ms/step - loss: 0.4352 - accuracy: 0.7989 - val_loss: 0.4102 - val_accuracy: 0.8125\n","Epoch 2/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.8256\n","Epoch 00002: val_loss improved from 0.41024 to 0.38200, saving model to ./models\\best_rmsprop3.epoch02-val_loss0.38.h5\n","1152/1152 [==============================] - 668s 580ms/step - loss: 0.3876 - accuracy: 0.8256 - val_loss: 0.3820 - val_accuracy: 0.8274\n","Epoch 3/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.8352\n","Epoch 00003: val_loss improved from 0.38200 to 0.37439, saving model to ./models\\best_rmsprop3.epoch03-val_loss0.37.h5\n","1152/1152 [==============================] - 663s 575ms/step - loss: 0.3698 - accuracy: 0.8352 - val_loss: 0.3744 - val_accuracy: 0.8321\n","Epoch 4/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.8419\n","Epoch 00004: val_loss improved from 0.37439 to 0.37283, saving model to ./models\\best_rmsprop3.epoch04-val_loss0.37.h5\n","1152/1152 [==============================] - 655s 569ms/step - loss: 0.3576 - accuracy: 0.8419 - val_loss: 0.3728 - val_accuracy: 0.8334\n","Epoch 5/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.3472 - accuracy: 0.8474\n","Epoch 00005: val_loss improved from 0.37283 to 0.37046, saving model to ./models\\best_rmsprop3.epoch05-val_loss0.37.h5\n","1152/1152 [==============================] - 656s 569ms/step - loss: 0.3472 - accuracy: 0.8474 - val_loss: 0.3705 - val_accuracy: 0.8345\n","Epoch 6/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.8523\n","Epoch 00006: val_loss did not improve from 0.37046\n","1152/1152 [==============================] - 655s 569ms/step - loss: 0.3379 - accuracy: 0.8523 - val_loss: 0.3731 - val_accuracy: 0.8338\n","Epoch 7/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.8568\n","Epoch 00007: val_loss did not improve from 0.37046\n","1152/1152 [==============================] - 658s 571ms/step - loss: 0.3299 - accuracy: 0.8568 - val_loss: 0.3870 - val_accuracy: 0.8337\n","Epoch 8/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.8610\n","Epoch 00008: val_loss did not improve from 0.37046\n","1152/1152 [==============================] - 655s 569ms/step - loss: 0.3221 - accuracy: 0.8610 - val_loss: 0.3970 - val_accuracy: 0.8301\n","Epoch 9/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.8648\n","Epoch 00009: val_loss did not improve from 0.37046\n","1152/1152 [==============================] - 668s 580ms/step - loss: 0.3143 - accuracy: 0.8648 - val_loss: 0.3849 - val_accuracy: 0.8309\n","Epoch 10/10\n","1152/1152 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8688\n","Epoch 00010: val_loss did not improve from 0.37046\n","1152/1152 [==============================] - 654s 568ms/step - loss: 0.3072 - accuracy: 0.8688 - val_loss: 0.4050 - val_accuracy: 0.8203\n"]}],"source":["epochs = 10\n","hist = model.fit(X_train, y_train, validation_data=(X_val,y_val), batch_size=1024, epochs=epochs, callbacks=callbacks)"]},{"cell_type":"code","source":["print('Train vs. Validation loss')\n","x = np.arange(0, len(hist.history['loss']))\n","plt.plot(x,hist.history['loss'], color='red', label='train')\n","plt.plot(x,hist.history['val_loss'], label='validation')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.legend(loc='upper right')\n","plt.show()\n","\n","print('\\nTrain vs. Validation Accuracy')\n","x = np.arange(0, len(hist.history['multi_acc']))\n","plt.plot(x,hist.history['multi_acc'], color='red', label='train')\n","plt.plot(x,hist.history['val_multi_acc'], label='validation')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"2Gl6Nx5cIunt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2P0fmv-7WPZ5"},"outputs":[],"source":["model = build_model()\n","best_model = './models/best_rmsprop3.epoch05-val_loss0.37.h5'\n","model.load_weights(best_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqRo8tYwWPZ5"},"outputs":[],"source":["'''\n","lstm Results with the evaluation set for kaggle\n","'''\n","df_eval = pd.read_csv(eval_path)\n","\n","df_eval = df_eval[['tweet_index','tweet']]\n","\n","# apply the translate function\n","df_eval['tweet'] = df_eval['tweet'].apply(translate)\n","tweets = df_eval['tweet']\n","\n","# apply tokener\n","data = tokener.texts_to_sequences(tweets)\n","data = pad_sequences(data, padding='post', maxlen=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPf8SwM8WPZ5"},"outputs":[],"source":["# predict\n","pred_eval =model.predict(data)\n","\n","preds = []\n","for pred in pred_eval:\n","    preds.append(np.argmax(pred))\n","    \n","out_data = {\n","    'tweet_index' : (df_eval['tweet_index']),\n","    'target' : (preds)\n","}\n","\n","out_df = pd.DataFrame(out_data, index=None)\n","\n","out_df.to_csv('./preds.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9vbqAAHWPZ5"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["QBBggRLnCN6N","LxHMjtYLmZ5Q","jLG9H7SpmsFQ","DaxzX_VomV1x"],"name":"Mini-Project-2_Blake&Ryan_Final-code.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}